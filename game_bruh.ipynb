{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cvzone\n",
    "import cv2\n",
    "from cvzone.HandTrackingModule import HandDetector\n",
    "import numpy as np\n",
    "import google.generativeai as genai\n",
    "from PIL import Image\n",
    "import pyttsx3\n",
    "import time\n",
    "import speech_recognition as sr\n",
    "import os\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "def seconds_since(initial_time):\n",
    "    return time.time() - initial_time\n",
    "time.sleep(5)\n",
    "print(round(seconds_since(start)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tree', 'House', 'Sun', 'Cloud', 'Flower', 'Car', 'Boat', 'Mountain', 'Bird', 'Fish', 'Cat', 'Dog', 'Butterfly', 'Star', 'Moon', 'Rainbow', 'Balloon', 'Ice Cream', 'Cupcake', 'Rocket']\n"
     ]
    }
   ],
   "source": [
    "# List of easily drawable real-life objects\n",
    "drawable_objects = [\n",
    "    \"Tree\",\n",
    "    \"House\",\n",
    "    \"Sun\",\n",
    "    \"Cloud\",\n",
    "    \"Flower\",\n",
    "    \"Car\",\n",
    "    \"Boat\",\n",
    "    \"Mountain\",\n",
    "    \"Bird\",\n",
    "    \"Fish\",\n",
    "    \"Cat\",\n",
    "    \"Dog\",\n",
    "    \"Butterfly\",\n",
    "    \"Star\",\n",
    "    \"Moon\",\n",
    "    \"Rainbow\",\n",
    "    \"Balloon\",\n",
    "    \"Ice Cream\",\n",
    "    \"Cupcake\",\n",
    "    \"Rocket\"\n",
    "]\n",
    "\n",
    "print(drawable_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Initialize the webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "start = time.time()\n",
    "\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    # Calculate elapsed time\n",
    "    elapsed_time = round(seconds_since(start))\n",
    "    \n",
    "    # Put the elapsed time on the frame\n",
    "    cv2.putText(frame, f'Time Elapsed: {elapsed_time} sec', (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "    \n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('Webcam', frame)\n",
    "    \n",
    "    # Break the loop on 'q' key press\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=\"wide\")\n",
    "# st.image('D:\\school projects\\idk\\cv proj\\main.py\\pepeCool.png')\n",
    " \n",
    "# col1, col2 = st.columns([3,2])\n",
    "# with col1:\n",
    "#    run = st.checkbox('Run', value=True)\n",
    "#    FRAME_WINDOW = st.image([])\n",
    " \n",
    "# with col2:\n",
    "#    st.title(\"Answer\")\n",
    "#    output_text_area = st.subheader(\"\")\n",
    "# import pyttsx3\n",
    "\n",
    "\n",
    "\n",
    "import customtkinter as ctk\n",
    "\n",
    "def show_modern_popup(message=\"Hello!\", title=\"Message\", width=800, height=400):\n",
    "    # Configure the appearance\n",
    "    ctk.set_appearance_mode(\"dark\")\n",
    "    ctk.set_default_color_theme(\"dark-blue\")\n",
    "    \n",
    "    # Create and configure the window\n",
    "    popup = ctk.CTk()\n",
    "    popup.title(title)\n",
    "    popup.geometry(f\"{width}x{height}\")\n",
    "    \n",
    "    # Center the window on screen\n",
    "    screen_width = popup.winfo_screenwidth()\n",
    "    screen_height = popup.winfo_screenheight()\n",
    "    center_x = int(screen_width/2 - width/2)\n",
    "    center_y = int(screen_height/2 - height/2)\n",
    "    popup.geometry(f'+{center_x}+{center_y}')\n",
    "    \n",
    "    # Add message label\n",
    "    label = ctk.CTkLabel(\n",
    "        popup,\n",
    "        text=message,\n",
    "        font=(\"Helvetica\", 14),\n",
    "        wraplength=width-40  # Wrap text if too long\n",
    "    )\n",
    "    label.pack(pady=20, padx=20, expand=True)\n",
    "    \n",
    "    # Add OK button\n",
    "    button = ctk.CTkButton(\n",
    "        popup,\n",
    "        text=\"OK\",\n",
    "        command=popup.destroy,\n",
    "        width=100,\n",
    "        height=32,\n",
    "        font=(\"Helvetica\", 12)\n",
    "    )\n",
    "    button.pack(pady=(0, 20))\n",
    "    \n",
    "    # Make window modal (user must interact with it)\n",
    "    popup.transient()\n",
    "    popup.grab_set()\n",
    "    \n",
    "    # Start the popup\n",
    "    popup.mainloop()\n",
    "\n",
    "# Example usage\n",
    "    \n",
    "# st.set_page_config(layout=\"wide\")\n",
    "# st.image('D:\\school projects\\idk\\cv proj\\main.py\\pepeCool.png')\n",
    " \n",
    "# col1, col2 = st.columns([3,2])\n",
    "# with col1:\n",
    "#    run = st.checkbox('Run', value=True)\n",
    "#    FRAME_WINDOW = st.image([])\n",
    " \n",
    "# with col2:\n",
    "#    st.title(\"Answer\")\n",
    "#    output_text_area = st.subheader(\"\")\n",
    " \n",
    " \n",
    "genai.configure(api_key=\"XXX\")\n",
    "model = genai.GenerativeModel('gemini-1.5-flash-002')\n",
    " \n",
    "# Initialize the webcam to capture video\n",
    "# The '2' indicates the third camera connected to your computer; '0' would usually refer to the built-in camera\n",
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(3,1280)\n",
    "cap.set(4,720)\n",
    " \n",
    "# Initialize the HandDetector class with the given parameters\n",
    "detector = HandDetector(staticMode=False, maxHands=1, modelComplexity=1, detectionCon=0.7, minTrackCon=0.5)\n",
    " \n",
    " \n",
    "def getHandInfo(img):\n",
    "    # Find hands in the current frame\n",
    "    # The 'draw' parameter draws landmarks and hand outlines on the image if set to True\n",
    "    # The 'flipType' parameter flips the image, making it easier for some detections\n",
    "    hands, img = detector.findHands(img, draw=False, flipType=True)\n",
    " \n",
    "    # Check if any hands are detected\n",
    "    if hands:\n",
    "        # Information for the first hand detected\n",
    "        hand = hands[0]  # Get the first hand detected\n",
    "        lmList = hand[\"lmList\"]  # List of 21 landmarks for the first hand\n",
    "        # Count the number of fingers up for the first hand\n",
    "        fingers = detector.fingersUp(hand)\n",
    "        #print(fingers)\n",
    "        return fingers, lmList\n",
    "    else:\n",
    "        return None\n",
    " \n",
    "def draw(info,prev_pos,canvas):\n",
    "    fingers, lmList = info\n",
    "    current_pos= None\n",
    "    if fingers == [1, 1, 0, 0, 0] or fingers ==[0,1,0,0,0]:\n",
    "        current_pos = lmList[8][0:2]\n",
    "        if prev_pos is None: prev_pos = current_pos\n",
    "        cv2.line(canvas,current_pos,prev_pos,(255,0,255),10)    \n",
    "    elif fingers == [1, 1, 1, 1, 1]:\n",
    "        canvas = np.zeros_like(img)\n",
    " \n",
    "    return current_pos, canvas\n",
    " \n",
    "def sendToAI(model, canvas, fingers, drawable_object, force_send=False):\n",
    "    if force_send:\n",
    "        pil_image = Image.fromarray(canvas)\n",
    "        response = model.generate_content([f\"You have to rate this hand drawing of the object {drawable_object} on a scale of 1 to 100, only return a number from 1 to 100, nothing else:\\n\", pil_image])\n",
    "        print(\"response is:\", response.text)\n",
    "        return response.text\n",
    "    elif fingers == [1, 1, 1, 0, 0]:\n",
    "        pil_image = Image.fromarray(canvas)\n",
    "        response = model.generate_content([f\"You have to rate this hand drawing of the object {drawable_object} on a scale of 1 to 100, only return a number from 1 to 100, nothing else:\\n\", pil_image])\n",
    "        print(\"response is:\", response.text)\n",
    "        return response.text\n",
    " \n",
    "\n",
    "\n",
    "def speech_to_text():\n",
    "    recognizer = sr.Recognizer()\n",
    "    with sr.Microphone() as source:\n",
    "        print(\"Listening...\")\n",
    "        audio = recognizer.listen(source)\n",
    "        try:\n",
    "            text = recognizer.recognize_google(audio)\n",
    "            print(\"You said: \" + text)\n",
    "            return text\n",
    "        except sr.UnknownValueError:\n",
    "            print(\"Google Speech Recognition could not understand audio\")\n",
    "            return None\n",
    "        except sr.RequestError as e:\n",
    "            print(\"Could not request results from Google Speech Recognition service; {0}\".format(e))\n",
    "            return None\n",
    " \n",
    "def text_to_speech(text):\n",
    "    engine = pyttsx3.init()\n",
    "    engine.say(text)\n",
    "    engine.runAndWait()\n",
    "\n",
    "def check_time_limit(start_time, limit=20):\n",
    "    elapsed_time = time.time() - start_time\n",
    "    return elapsed_time >= limit\n",
    "\n",
    "prev_pos = None\n",
    "canvas=None\n",
    "image_combined = None\n",
    "output_text= \"\"\n",
    "prev_output = None\n",
    "# Continuously get frames from the webcam\n",
    "time_limit = 21\n",
    "start = time.time()\n",
    "drawable_object = np.random.choice(drawable_objects)\n",
    "alreadysent = False\n",
    "while True:\n",
    "    # Capture each frame from the webcam\n",
    "    # 'success' will be True if the frame is successfully captured, 'img' will contain the frame\n",
    "    success, img = cap.read()\n",
    "    img = cv2.flip(img, 1)\n",
    " \n",
    "    if canvas is None:\n",
    "        canvas = np.zeros_like(img)\n",
    " \n",
    " \n",
    "    info = getHandInfo(img)\n",
    "    if info:\n",
    "        fingers, lmList = info\n",
    "        prev_pos,canvas = draw(info, prev_pos,canvas)\n",
    "        # if fingers == [1, 1, 1, 0, 0]:\n",
    "        #     output_text = sendToAI(model,canvas,fingers,drawable_object=drawable_object)\n",
    "        #     print(output_text)\n",
    "        if fingers == [1, 1, 1, 0, 0]:\n",
    "            canvas = np.zeros_like(img)\n",
    "            drawable_object = np.random.choice(drawable_objects)\n",
    "            start = time.time()\n",
    "            output_text = None\n",
    "            \n",
    "                \n",
    "        if check_time_limit(start, time_limit) and alreadysent == False:\n",
    "            output_text = sendToAI(model,canvas,fingers,drawable_object=drawable_object, force_send=True) \n",
    "            print(output_text)\n",
    "            alreadysent = True\n",
    "    image_combined= cv2.addWeighted(img,0.7,canvas,0.3,0)\n",
    "    #FRAME_WINDOW.image(image_combined,channels=\"BGR\")  \n",
    "    # Calculate elapsed time\n",
    "    elapsed_time = time.time() - start\n",
    "\n",
    "    # Display elapsed time and output text if available\n",
    "    # Display elapsed time\n",
    "    time_left = max(0, time_limit - int(elapsed_time))\n",
    "    if time_left == 0:\n",
    "        time_left = 1\n",
    "    cv2.putText(image_combined, f'Time Left: {time_left-1} sec', (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "    # Display the drawable object\n",
    "    cv2.putText(image_combined, f'Draw: {drawable_object}', (10, 110), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "    # Display output text if available\n",
    "    if output_text:\n",
    "        cv2.putText(image_combined, f'Score: {output_text.strip()[:2]}%', (10, 70), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "    \n",
    "     # Display the image in a window\n",
    "   #cv2.imshow(\"Image\", img)\n",
    "    #cv2.imshow(\"Canvas\", canvas)\n",
    "    cv2.imshow(\"image_combined\", image_combined)\n",
    " \n",
    " \n",
    "    # Keep the window open and update it for each frame; wait for 1 millisecond between frames\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        #\n",
    "    \n",
    "        #    pass\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'72'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'72'[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cvzone\n",
    "import cv2\n",
    "from cvzone.HandTrackingModule import HandDetector\n",
    "import numpy as np\n",
    "import google.generativeai as genai\n",
    "from PIL import Image\n",
    "import streamlit as st\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import speech_recognition as sr\n",
    "\n",
    "import pyttsx3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-22 17:59:54.107 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-22 17:59:54.109 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-22 17:59:54.113 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-22 17:59:54.116 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-22 17:59:54.119 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-22 17:59:54.121 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-22 17:59:54.123 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-22 17:59:54.125 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-22 17:59:54.126 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-22 17:59:54.128 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-22 17:59:54.130 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-22 17:59:54.133 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-22 17:59:54.135 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-22 17:59:54.137 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-22 17:59:54.139 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-22 17:59:54.142 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-22 17:59:54.144 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-22 17:59:54.146 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-22 17:59:54.147 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "st.set_page_config(layout=\"wide\")\n",
    "st.image('D:\\school projects\\idk\\cv proj\\main.py\\pepeCool.png')\n",
    " \n",
    "col1, col2 = st.columns([3,2])\n",
    "with col1:\n",
    "   run = st.checkbox('Run', value=True)\n",
    "   FRAME_WINDOW = st.image([])\n",
    " \n",
    "with col2:\n",
    "   st.title(\"Answer\")\n",
    "   output_text_area = st.subheader(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listening...\n",
      "You said: find the area of this rectangle\n",
      "Here's how to solve this:\n",
      "\n",
      "**1. Identify the dimensions:**\n",
      "\n",
      "The image shows a rectangle with a width of 5 and a height of 10.\n",
      "\n",
      "**2. Calculate the area:**\n",
      "\n",
      "The area of a rectangle is found by multiplying its width and height.  Therefore:\n",
      "\n",
      "Area = width × height = 5 × 10 = 50\n",
      "\n",
      "**Answer:** The area of the rectangle is 50 square units.\n",
      "Listening...\n",
      "You said: find the area of the circle\n",
      "Here's how to solve this:\n",
      "\n",
      "**1. Identify the radius:**\n",
      "\n",
      "The image shows a line segment within the circle labeled \"5\".  This line segment represents the radius (r) of the circle.  Therefore, r = 5 units.\n",
      "\n",
      "**2. Use the area formula:**\n",
      "\n",
      "The area (A) of a circle is given by the formula: A = πr²\n",
      "\n",
      "**3. Calculate the area:**\n",
      "\n",
      "Substitute the radius (r = 5) into the formula:\n",
      "\n",
      "A = π * 5² = 25π\n",
      "\n",
      "**4. Approximate the answer:**\n",
      "\n",
      "Using the approximation π ≈ 3.14159:\n",
      "\n",
      "A ≈ 25 * 3.14159 ≈ 78.53975 square units\n",
      "\n",
      "Therefore, the area of the circle is approximately 78.54 square units.\n",
      "Listening...\n",
      "You said: nothing\n",
      "The answer is 0.  There is no mathematical question or problem provided.\n",
      "Listening...\n",
      "Google Speech Recognition could not understand audio\n",
      "Could not understand the query.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)\n",
      "Cell \u001b[1;32mIn[14], line 160\u001b[0m\n",
      "\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m canvas \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;32m    157\u001b[0m     canvas \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros_like(img)\n",
      "\u001b[1;32m--> 160\u001b[0m info \u001b[38;5;241m=\u001b[39m \u001b[43mgetHandInfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m info:\n",
      "\u001b[0;32m    162\u001b[0m     fingers, lmList \u001b[38;5;241m=\u001b[39m info\n",
      "\n",
      "Cell \u001b[1;32mIn[14], line 83\u001b[0m, in \u001b[0;36mgetHandInfo\u001b[1;34m(img)\u001b[0m\n",
      "\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgetHandInfo\u001b[39m(img):\n",
      "\u001b[0;32m     80\u001b[0m     \u001b[38;5;66;03m# Find hands in the current frame\u001b[39;00m\n",
      "\u001b[0;32m     81\u001b[0m     \u001b[38;5;66;03m# The 'draw' parameter draws landmarks and hand outlines on the image if set to True\u001b[39;00m\n",
      "\u001b[0;32m     82\u001b[0m     \u001b[38;5;66;03m# The 'flipType' parameter flips the image, making it easier for some detections\u001b[39;00m\n",
      "\u001b[1;32m---> 83\u001b[0m     hands, img \u001b[38;5;241m=\u001b[39m \u001b[43mdetector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfindHands\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdraw\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflipType\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m     85\u001b[0m     \u001b[38;5;66;03m# Check if any hands are detected\u001b[39;00m\n",
      "\u001b[0;32m     86\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hands:\n",
      "\u001b[0;32m     87\u001b[0m         \u001b[38;5;66;03m# Information for the first hand detected\u001b[39;00m\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\cvzone\\HandTrackingModule.py:55\u001b[0m, in \u001b[0;36mHandDetector.findHands\u001b[1;34m(self, img, draw, flipType)\u001b[0m\n",
      "\u001b[0;32m     48\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
      "\u001b[0;32m     49\u001b[0m \u001b[38;5;124;03mFinds hands in a BGR image.\u001b[39;00m\n",
      "\u001b[0;32m     50\u001b[0m \u001b[38;5;124;03m:param img: Image to find the hands in.\u001b[39;00m\n",
      "\u001b[0;32m     51\u001b[0m \u001b[38;5;124;03m:param draw: Flag to draw the output on the image.\u001b[39;00m\n",
      "\u001b[0;32m     52\u001b[0m \u001b[38;5;124;03m:return: Image with or without drawings\u001b[39;00m\n",
      "\u001b[0;32m     53\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
      "\u001b[0;32m     54\u001b[0m imgRGB \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(img, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB)\n",
      "\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhands\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgRGB\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m     56\u001b[0m allHands \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;32m     57\u001b[0m h, w, c \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mshape\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\mediapipe\\python\\solutions\\hands.py:153\u001b[0m, in \u001b[0;36mHands.process\u001b[1;34m(self, image)\u001b[0m\n",
      "\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess\u001b[39m(\u001b[38;5;28mself\u001b[39m, image: np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NamedTuple:\n",
      "\u001b[0;32m    133\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Processes an RGB image and returns the hand landmarks and handedness of each detected hand.\u001b[39;00m\n",
      "\u001b[0;32m    134\u001b[0m \n",
      "\u001b[0;32m    135\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n",
      "\u001b[1;32m   (...)\u001b[0m\n",
      "\u001b[0;32m    150\u001b[0m \u001b[38;5;124;03m         right hand) of the detected hand.\u001b[39;00m\n",
      "\u001b[0;32m    151\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n",
      "\u001b[1;32m--> 153\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\mediapipe\\python\\solution_base.py:372\u001b[0m, in \u001b[0;36mSolutionBase.process\u001b[1;34m(self, input_data)\u001b[0m\n",
      "\u001b[0;32m    366\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;32m    367\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph\u001b[38;5;241m.\u001b[39madd_packet_to_input_stream(\n",
      "\u001b[0;32m    368\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream_name,\n",
      "\u001b[0;32m    369\u001b[0m         packet\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_packet(input_stream_type,\n",
      "\u001b[0;32m    370\u001b[0m                                  data)\u001b[38;5;241m.\u001b[39mat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_simulated_timestamp))\n",
      "\u001b[1;32m--> 372\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_graph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait_until_idle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m    373\u001b[0m \u001b[38;5;66;03m# Create a NamedTuple object where the field names are mapping to the graph\u001b[39;00m\n",
      "\u001b[0;32m    374\u001b[0m \u001b[38;5;66;03m# output stream names.\u001b[39;00m\n",
      "\u001b[0;32m    375\u001b[0m solution_outputs \u001b[38;5;241m=\u001b[39m collections\u001b[38;5;241m.\u001b[39mnamedtuple(\n",
      "\u001b[0;32m    376\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSolutionOutputs\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_stream_type_info\u001b[38;5;241m.\u001b[39mkeys())\n",
      "\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pyttsx3\n",
    "\n",
    "\n",
    "\n",
    "import customtkinter as ctk\n",
    "\n",
    "def show_modern_popup(message=\"Hello!\", title=\"Message\", width=800, height=400):\n",
    "    # Configure the appearance\n",
    "    ctk.set_appearance_mode(\"dark\")\n",
    "    ctk.set_default_color_theme(\"dark-blue\")\n",
    "    \n",
    "    # Create and configure the window\n",
    "    popup = ctk.CTk()\n",
    "    popup.title(title)\n",
    "    popup.geometry(f\"{width}x{height}\")\n",
    "    \n",
    "    # Center the window on screen\n",
    "    screen_width = popup.winfo_screenwidth()\n",
    "    screen_height = popup.winfo_screenheight()\n",
    "    center_x = int(screen_width/2 - width/2)\n",
    "    center_y = int(screen_height/2 - height/2)\n",
    "    popup.geometry(f'+{center_x}+{center_y}')\n",
    "    \n",
    "    # Add message label\n",
    "    label = ctk.CTkLabel(\n",
    "        popup,\n",
    "        text=message,\n",
    "        font=(\"Helvetica\", 14),\n",
    "        wraplength=width-40  # Wrap text if too long\n",
    "    )\n",
    "    label.pack(pady=20, padx=20, expand=True)\n",
    "    \n",
    "    # Add OK button\n",
    "    button = ctk.CTkButton(\n",
    "        popup,\n",
    "        text=\"OK\",\n",
    "        command=popup.destroy,\n",
    "        width=100,\n",
    "        height=32,\n",
    "        font=(\"Helvetica\", 12)\n",
    "    )\n",
    "    button.pack(pady=(0, 20))\n",
    "    \n",
    "    # Make window modal (user must interact with it)\n",
    "    popup.transient()\n",
    "    popup.grab_set()\n",
    "    \n",
    "    # Start the popup\n",
    "    popup.mainloop()\n",
    "\n",
    "# Example usage\n",
    "    \n",
    "# st.set_page_config(layout=\"wide\")\n",
    "# st.image('D:\\school projects\\idk\\cv proj\\main.py\\pepeCool.png')\n",
    " \n",
    "# col1, col2 = st.columns([3,2])\n",
    "# with col1:\n",
    "#    run = st.checkbox('Run', value=True)\n",
    "#    FRAME_WINDOW = st.image([])\n",
    " \n",
    "# with col2:\n",
    "#    st.title(\"Answer\")\n",
    "#    output_text_area = st.subheader(\"\")\n",
    " \n",
    " \n",
    "genai.configure(api_key=\"XXX\")\n",
    "model = genai.GenerativeModel('gemini-1.5-flash-002')\n",
    " \n",
    "# Initialize the webcam to capture video\n",
    "# The '2' indicates the third camera connected to your computer; '0' would usually refer to the built-in camera\n",
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(3,1280)\n",
    "cap.set(4,720)\n",
    " \n",
    "# Initialize the HandDetector class with the given parameters\n",
    "detector = HandDetector(staticMode=False, maxHands=1, modelComplexity=1, detectionCon=0.7, minTrackCon=0.5)\n",
    " \n",
    " \n",
    "def getHandInfo(img):\n",
    "    # Find hands in the current frame\n",
    "    # The 'draw' parameter draws landmarks and hand outlines on the image if set to True\n",
    "    # The 'flipType' parameter flips the image, making it easier for some detections\n",
    "    hands, img = detector.findHands(img, draw=False, flipType=True)\n",
    " \n",
    "    # Check if any hands are detected\n",
    "    if hands:\n",
    "        # Information for the first hand detected\n",
    "        hand = hands[0]  # Get the first hand detected\n",
    "        lmList = hand[\"lmList\"]  # List of 21 landmarks for the first hand\n",
    "        # Count the number of fingers up for the first hand\n",
    "        fingers = detector.fingersUp(hand)\n",
    "        #print(fingers)\n",
    "        return fingers, lmList\n",
    "    else:\n",
    "        return None\n",
    " \n",
    "def draw(info,prev_pos,canvas):\n",
    "    fingers, lmList = info\n",
    "    current_pos= None\n",
    "    if fingers == [1, 1, 0, 0, 0] or fingers ==[0,1,0,0,0]:\n",
    "        current_pos = lmList[8][0:2]\n",
    "        if prev_pos is None: prev_pos = current_pos\n",
    "        cv2.line(canvas,current_pos,prev_pos,(255,0,255),10)    \n",
    "    elif fingers == [0, 0, 0, 0, 1]:\n",
    "        canvas = np.zeros_like(img)\n",
    " \n",
    "    return current_pos, canvas\n",
    " \n",
    "def sendToAI(model, canvas, fingers):\n",
    "    if fingers == [1, 1, 1, 0, 0]:\n",
    "        pil_image = Image.fromarray(canvas)\n",
    "        text_to_speech(\"ask your question\")\n",
    "        user_query = speech_to_text()\n",
    "        if user_query:\n",
    "            text_to_speech(\"i am solving the problem\")\n",
    "            response = model.generate_content([f\"Answer this maths question: {user_query}\", pil_image])\n",
    "            return response.text\n",
    "        else:\n",
    "            return \"Could not understand the query.\"\n",
    " \n",
    "\n",
    "\n",
    "def speech_to_text():\n",
    "    recognizer = sr.Recognizer()\n",
    "    with sr.Microphone() as source:\n",
    "        print(\"Listening...\")\n",
    "        audio = recognizer.listen(source)\n",
    "        try:\n",
    "            text = recognizer.recognize_google(audio)\n",
    "            print(\"You said: \" + text)\n",
    "            return text\n",
    "        except sr.UnknownValueError:\n",
    "            print(\"Google Speech Recognition could not understand audio\")\n",
    "            return None\n",
    "        except sr.RequestError as e:\n",
    "            print(\"Could not request results from Google Speech Recognition service; {0}\".format(e))\n",
    "            return None\n",
    " \n",
    "def text_to_speech(text):\n",
    "    engine = pyttsx3.init()\n",
    "    engine.say(text)\n",
    "    engine.runAndWait()\n",
    "\n",
    "prev_pos = None\n",
    "canvas=None\n",
    "image_combined = None\n",
    "output_text= \"\"\n",
    "prev_output = None\n",
    "# Continuously get frames from the webcam\n",
    "while True:\n",
    "    # Capture each frame from the webcam\n",
    "    # 'success' will be True if the frame is successfully captured, 'img' will contain the frame\n",
    "    success, img = cap.read()\n",
    "    img = cv2.flip(img, 1)\n",
    " \n",
    "    if canvas is None:\n",
    "        canvas = np.zeros_like(img)\n",
    " \n",
    " \n",
    "    info = getHandInfo(img)\n",
    "    if info:\n",
    "        fingers, lmList = info\n",
    "        prev_pos,canvas = draw(info, prev_pos,canvas)\n",
    "        output_text = sendToAI(model,canvas,fingers)\n",
    " \n",
    "    image_combined= cv2.addWeighted(img,0.7,canvas,0.3,0)\n",
    "    #FRAME_WINDOW.image(image_combined,channels=\"BGR\")\n",
    " \n",
    "    if output_text != prev_output:\n",
    "        if output_text != None and output_text != \"\":\n",
    "            #output_text_area.text(output_text)\n",
    "            print(output_text)\n",
    "            show_modern_popup(\n",
    "            message=output_text,\n",
    "            title=\"Answer\"\n",
    "        )\n",
    "            #text_to_speech(output_text)\n",
    "        prev_output = output_text\n",
    "    \n",
    "     # Display the image in a window\n",
    "   #cv2.imshow(\"Image\", img)\n",
    "    #cv2.imshow(\"Canvas\", canvas)\n",
    "    cv2.imshow(\"image_combined\", image_combined)\n",
    " \n",
    " \n",
    "    # Keep the window open and update it for each frame; wait for 1 millisecond between frames\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        #   pass\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!streamlit run C:\\Users\\ADMIN\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel_launcher.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import customtkinter as ctk\n",
    "\n",
    "def show_modern_popup(message=\"Hello!\", title=\"Message\", width=400, height=200):\n",
    "    # Configure the appearance\n",
    "    ctk.set_appearance_mode(\"dark\")\n",
    "    ctk.set_default_color_theme(\"dark-blue\")\n",
    "    \n",
    "    # Create and configure the window\n",
    "    popup = ctk.CTk()\n",
    "    popup.title(title)\n",
    "    popup.geometry(f\"{width}x{height}\")\n",
    "    \n",
    "    # Center the window on screen\n",
    "    screen_width = popup.winfo_screenwidth()\n",
    "    screen_height = popup.winfo_screenheight()\n",
    "    center_x = int(screen_width/2 - width/2)\n",
    "    center_y = int(screen_height/2 - height/2)\n",
    "    popup.geometry(f'+{center_x}+{center_y}')\n",
    "    \n",
    "    # Add message label\n",
    "    label = ctk.CTkLabel(\n",
    "        popup,\n",
    "        text=message,\n",
    "        font=(\"Helvetica\", 14),\n",
    "        wraplength=width-40  # Wrap text if too long\n",
    "    )\n",
    "    label.pack(pady=20, padx=20, expand=True)\n",
    "    \n",
    "    # Add OK button\n",
    "    button = ctk.CTkButton(\n",
    "        popup,\n",
    "        text=\"OK\",\n",
    "        command=popup.destroy,\n",
    "        width=100,\n",
    "        height=32,\n",
    "        font=(\"Helvetica\", 12)\n",
    "    )\n",
    "    button.pack(pady=(0, 20))\n",
    "    \n",
    "    # Make window modal (user must interact with it)\n",
    "    popup.transient()\n",
    "    popup.grab_set()\n",
    "    \n",
    "    # Start the popup\n",
    "    popup.mainloop()\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    show_modern_popup(\n",
    "        message=\"This is a modern dark-themed popup with a clean design!\",\n",
    "        title=\"Modern Popup\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "def speech_to_text():\n",
    "    recognizer = sr.Recognizer()\n",
    "    with sr.Microphone() as source:\n",
    "        print(\"Listening...\")\n",
    "        audio = recognizer.listen(source)\n",
    "        try:\n",
    "            text = recognizer.recognize_google(audio)\n",
    "            print(\"You said: \" + text)\n",
    "            return text\n",
    "        except sr.UnknownValueError:\n",
    "            print(\"Google Speech Recognition could not understand audio\")\n",
    "            return None\n",
    "        except sr.RequestError as e:\n",
    "            print(\"Could not request results from Google Speech Recognition service; {0}\".format(e))\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import messagebox\n",
    "\n",
    "# Function to show the pop-up\n",
    "def show_popup():\n",
    "    messagebox.showinfo(\"Popup\", \"This is a pop-up with text!\")\n",
    "\n",
    "# Create the main window\n",
    "root = tk.Tk()\n",
    "root.withdraw()  # Hide the root window\n",
    "\n",
    "# Show the pop-up\n",
    "show_popup()\n",
    "\n",
    "# Keep the window open\n",
    "root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
